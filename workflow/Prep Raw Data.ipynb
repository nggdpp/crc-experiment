{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Steps and Discussion\n",
    "The \"raw\" data I used for building new ScienceBase Items for the Core Research Center cores and cuttings collections started with downloads from the CRC Well Catalog web application. The data downloaded as CSV from this process is relatively harmonized between the two collections and represents the best online availability for the data at this time. After experimenting with various ways of reading files into memory for all processing steps, I opted to use a local MongoDB instance in Docker as an assembly point. Information assembled from the Macrostrat API on geologic map unit context if keyed on latitude and longitude coordinates pulled from the raw data. To support the later assembly of these related properties, I run a process here to generate a geohash string from the coordinates. This gives me a single unique value to operate against in later steps.\n",
    "\n",
    "This code executes the following steps for each collection:\n",
    "\n",
    "1. Read the CSV file from a dynamic web server response into a Pandas dataframe\n",
    "2. Geohash the coordinates into a new field and add two additional preset values (string for the type of collection and the target ScienceBase Item ID for the relevant collection that the items are destined for)\n",
    "3. Load the resulting dataframe as a list of dictionaries to MongoDB collections for the \"raw\" input data\n",
    "\n",
    "The raw data downloaded and prepped here contains duplicate records for the actual core and cutting metadata where there are multiple borehole intervals identified in the data. For ScienceBase purposes, we want to only identify one physical sample item for each core and cutting metadata record, grouping together interval information into an array. Because we brought the data together into MongoDB in this step, we can accomplish that grouping at the end using aggregation in the database.\n",
    "\n",
    "# Dependencies\n",
    "This code requires the Geohash2 and Pandas (installed from Conda-Forge distributions in my case). I use MongoDB with a local instance run in Docker from DockerHub with no authentication. This sets up barebones instance that does what I need it to do and can then go away. The same process could be run with a variety of different approaches from other databases to local file storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import geohash2\n",
    "import pandas as pd\n",
    "\n",
    "mongo_ndc = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geohash_coords(lat, lng):\n",
    "    if lat is None:\n",
    "        return None\n",
    "    else:\n",
    "        return geohash2.encode(float(lat), float(lng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 s, sys: 105 ms, total: 1.56 s\n",
      "Wall time: 53.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x11c352f88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Set raw data download from CRC web site\n",
    "cores_raw_url = \"https://my.usgs.gov/crcwc/search/cores?f=csv&extension=csv&offset=0&max=50&county=&format=&section=&wellname=&formation=&type=&operator=&cuttings=true&search=Search&cores=true&field=&crclibrarynumber=&townshipnumber=&state=&apinumber=&rangenumber=&fieldsorting=%2Btwnnum&fieldsorting=%2Blibnum&fieldsorting=%2Bmindepth\"\n",
    "\n",
    "# Read raw data CSV into dataframe\n",
    "df_cores_raw = pd.read_csv(cores_raw_url, dtype=str)\n",
    "\n",
    "# Add geohashed coordinates and two default properties to dataframe\n",
    "df_cores_raw[\"coordinates_geohash\"] = df_cores_raw.apply(lambda x: geohash_coords(x[\"Latitude\"], x[\"Longitude\"]), axis=1)\n",
    "df_cores_raw[\"sb_parent_id\"] = \"4f4e49dae4b07f02db5e0486\"\n",
    "df_cores_raw[\"crc_collection_name\"] = \"core\"\n",
    "\n",
    "# Load list of JSON documents to MongoDB collection for processing changing NaN to None values\n",
    "mongo_ndc.crc.cores_raw.insert_many(df_cores_raw.where((pd.notnull(df_cores_raw)), None).to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.89 s, sys: 316 ms, total: 5.21 s\n",
      "Wall time: 2min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x11e507e08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Set raw data download from CRC web site\n",
    "cuttings_raw_url = \"https://my.usgs.gov/crcwc/search/cuttings?f=csv&extension=csv&offset=0&max=50&county=&format=&section=&wellname=&formation=&type=&operator=&cuttings=true&search=Search&cores=true&field=&crclibrarynumber=&townshipnumber=&state=&apinumber=&rangenumber=&fieldsorting=%5B%2Btwnnum%2C+%2Blibnum%2C+%2Bmindepth%5D\"\n",
    "\n",
    "# Read raw data CSV into dataframe\n",
    "df_cuttings_raw = pd.read_csv(cuttings_raw_url, dtype=str)\n",
    "\n",
    "# Add geohashed coordinates and two default properties to dataframe\n",
    "df_cuttings_raw[\"coordinates_geohash\"] = df_cuttings_raw.apply(lambda x: geohash_coords(x[\"Latitude\"], x[\"Longitude\"]), axis=1)\n",
    "df_cuttings_raw[\"sb_parent_id\"] = \"4f4e49d8e4b07f02db5df2d2\"\n",
    "df_cuttings_raw[\"crc_collection_name\"] = \"cutting\"\n",
    "\n",
    "# Load list of JSON documents to MongoDB collection for processing changing NaN to None values\n",
    "mongo_ndc.crc.cuttings_raw.insert_many(df_cuttings_raw.where((pd.notnull(df_cuttings_raw)), None).to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
