{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Steps and Discussion\n",
    "The MapServer data gave us the necessary ID values to use in constructing landing page URLs for the cores and cuttings. The landing pages contain information on thin sections, documents, and photos. Out of the 62K total items, about 10K of those have one or more related artifacts based on the \"T/F\" values from the core/cutting metadata. We can put the URLs we generated from the internal id property into a list and send those to a function that uses BeautifulSoup to pull out the relevant bits into a data structure that we can then stitch together into our item information.\n",
    "\n",
    "As I first started trying to run this, I was attempting to simply run everything in real time to assemble my final items on the fly. I kept running into issues with HTTP disconnects and other hiccups. This happens for all kinds of reasons, but it's a pain when trying to build out something like this - essentially, a distributed data system that is using the web and linking parameters as a relational database. These processes blow up routinely with a connection aborted error, potentially due to something happening at an edge firewall device that doesn't like receiving so many connections.\n",
    "\n",
    "Included in the raw data properties are boolean indicators of whether or not a given core/cutting record has related photos, \"analysis\" files, and/or thin sections. Now that we have everything in MongoDB, it makes sense to leverage a bit of the aggregation framework to pull together the records we want to operate against, those that will have something usable on the web pages to scrape.\n",
    "\n",
    "## Dependencies\n",
    "This notebook requires the Requests, BeautifulSoup4, and Pymongo clients, all from Conda-Forge distributions in my case. I also use the builtin datetime to stamp each record extracted with the date/time it was scraped. This could help determine when an update should be run in future. More than other steps in this workflow, this one involves some relatively heavy MongdoDB dependencies. Once we start assembling our data in the local Docker-based MongoDB, it ends up being much more efficient to conduct certain operations at the database instead of pulling the data down and then pushing it back in some other form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "mongo_ndc = MongoClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Function\n",
    "BeautifulSoup is one of a number of available tools for scraping data from web pages. In examining the HTML content of the CRC core and cutting \"report\" pages, there are a couple of usable hooks that let us zero in on the section of the dynamically rendered pages that have links to photos and documents or sets of a table of thin section information. This function is, of course, completely specific to this particular use case. However, it may be a useful pattern for other cases on the way toward providing a more usable data structure behind the various collections we are working with in the data preservation pursuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crc_landing_page(crcwc_url):\n",
    "    target_schemas = {\n",
    "        \"intervals\": ['Min Depth', 'Max Depth', 'Age', 'Formation'],\n",
    "        \"thin_sections\": ['Sequence', 'Min Depth', 'Max Depth', 'View']\n",
    "    }\n",
    "    \n",
    "    r = requests.get(crcwc_url)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    data_structures = {\n",
    "        \"crcwc_url\": crcwc_url,\n",
    "        \"date_scraped\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "    for index, table in enumerate(soup.findAll(\"table\",{\"class\":\"report2\"})):\n",
    "        first_row = table.find(\"tr\")\n",
    "        labels = [i.text for i in first_row.findAll(\"td\", {\"class\": \"label\"})]\n",
    "        target_data = list(target_schemas.keys())[list(target_schemas.values()).index(labels)]\n",
    "        data_structures[target_data] = list()\n",
    "\n",
    "        for row in [r for r in table.findAll(\"tr\")][1:]:\n",
    "            d_this = dict()\n",
    "            for i, col in enumerate(row.findAll(\"td\")):\n",
    "                anchor = col.find(\"a\")\n",
    "                if anchor:\n",
    "                    this_data = anchor.get(\"href\")\n",
    "                else:\n",
    "                    this_data = col.text\n",
    "                d_this[labels[i]] = this_data\n",
    "            data_structures[target_data].append(d_this)\n",
    "\n",
    "    photos = list()\n",
    "    documents = list()\n",
    "\n",
    "    for section in soup.findAll(\"div\",{\"class\":\"report2\"}):\n",
    "        photos.extend(list(set([i.get('href') for i in section.findAll(\"a\",{\"title\":\"see photo\"})])))\n",
    "        documents.extend(list(set([i.get('href') for i in section.findAll(\"a\",{\"title\":\"download analysis document\"})])))\n",
    "        \n",
    "    if len(photos) > 0:\n",
    "        data_structures[\"photos\"] = photos\n",
    "        \n",
    "    if len(documents) > 0:\n",
    "        data_structures[\"documents\"] = documents\n",
    "        \n",
    "    if not data_structures:\n",
    "        return None\n",
    "    else:\n",
    "        return data_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations to Pull Usable Landing Page Links\n",
    "The following two aggregation pipelines implement the necessary logic to put together lists of cores and cuttings that will have something harvestable from their web pages and the links to those pages from our previous pull of identifier values from MapServer. To help deal with the vagaries of the web and needing to potentially restart this process multiple times, I set up essentially a ledger of \"orders\" to be filled with the web scraping routine. Each ledger entry has a URL and a CRC Library Number to go after. The order is filled when the web scraping function runs, pulls its information together, and updates the MongoDB record with a data stamp and the resulting information. We can essentially run it while any record doesn't have its date stamp filled in. The pipelines here finish by dumping their results into two different collections, but we really only need one \"ledger\" to operate against. I finish this part of the process by copying all the items from the second collection into the first one and dropping the second collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_url_pipeline = [\n",
    "    {\n",
    "        u\"$match\": {\n",
    "            u\"$or\": [\n",
    "                {\n",
    "                    u\"Photos\": u\"T\"\n",
    "                },\n",
    "                {\n",
    "                    u\"Thin Sec\": u\"T\"\n",
    "                },\n",
    "                {\n",
    "                    u\"Analysis\": u\"T\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$group\": {\n",
    "            u\"_id\": u\"$Lib Num\"\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$lookup\": {\n",
    "            u\"from\": u\"cores_from_mapserver\",\n",
    "            u\"localField\": u\"_id\",\n",
    "            u\"foreignField\": u\"properties.libno\",\n",
    "            u\"as\": u\"mapserver_data\"\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$project\": {\n",
    "            u\"_id\": 0.0,\n",
    "            u\"Lib Num\": u\"$_id\",\n",
    "            u\"crcwc_url\": {\n",
    "                u\"$concat\": [\n",
    "                    u\"https://my.usgs.gov/crcwc/core/report/\",\n",
    "                    {\n",
    "                        u\"$toString\": {\n",
    "                            u\"$arrayElemAt\": [\n",
    "                                u\"$mapserver_data.id\",\n",
    "                                0.0\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$out\": u\"web_page_info\"\n",
    "    }\n",
    "]\n",
    "\n",
    "cutting_url_pipeline = [\n",
    "    {\n",
    "        u\"$match\": {\n",
    "            u\"$or\": [\n",
    "                {\n",
    "                    u\"Thin Sec\": u\"T\"\n",
    "                },\n",
    "                {\n",
    "                    u\"Analysis\": u\"T\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$group\": {\n",
    "            u\"_id\": u\"$Lib Num\"\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$lookup\": {\n",
    "            u\"from\": u\"cuttings_from_mapserver\",\n",
    "            u\"localField\": u\"_id\",\n",
    "            u\"foreignField\": u\"properties.chlibno\",\n",
    "            u\"as\": u\"mapserver_data\"\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$project\": {\n",
    "            u\"_id\": 0.0,\n",
    "            u\"Lib Num\": u\"$_id\",\n",
    "            u\"crcwc_url\": {\n",
    "                u\"$concat\": [\n",
    "                    u\"https://my.usgs.gov/crcwc/cutting/report/\",\n",
    "                    {\n",
    "                        u\"$toString\": {\n",
    "                            u\"$arrayElemAt\": [\n",
    "                                u\"$mapserver_data.id\",\n",
    "                                0.0\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        u\"$out\": u\"web_page_info_cuttings\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codeblock executes both URL ledger-building pipelines to build new collections, assembles the one final collection, and drops the one we don't need to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.1 ms, sys: 10.3 ms, total: 57.4 ms\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mongo_ndc.crc.cores_raw.aggregate(core_url_pipeline)\n",
    "mongo_ndc.crc.cuttings_raw.aggregate(cutting_url_pipeline)\n",
    "mongo_ndc.crc.web_page_info.insert_many([i for i in mongo_ndc.crc.web_page_info_cuttings.find({},{\"_id\": 0})])\n",
    "mongo_ndc.crc.web_page_info_cuttings.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of all the prep work, the rest is reasonably simple - running the ledger for everything not yet date stamped, running the scraper, and updating the ledger with the extracted information. This codeblock had to be restarted a number of times to fully complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = mongo_ndc.crc.web_page_info.find_one({\"date_scraped\": {\"$exists\": False}}, sort=[('crcwc_url', -1)])\n",
    "while item is not None:\n",
    "    mongo_ndc.crc.web_page_info.update_one({\"_id\": item[\"_id\"]},{\"$set\": extract_crc_landing_page(item[\"crcwc_url\"])})\n",
    "    item = mongo_ndc.crc.web_page_info.find_one({\"date_scraped\": {\"$exists\": False}}, sort=[('crcwc_url', -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
