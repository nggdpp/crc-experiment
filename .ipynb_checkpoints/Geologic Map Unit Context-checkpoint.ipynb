{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Steps and Discussion\n",
    "As part of this experiment, I looked at what information we might retrieve via the very usable [Macrostrat API](http://macrostrat.org/api). There are a number of different routes that could bring useful context to the point locations in the NDC, and we would eventually want to run this as an \"enhancer\" on the entire catalog. For now, I worked up a process to use one of the simplest API routes set up for mobile applications to return some basic geologic map unit context for surface geology. This gives us rock type, geologic formation, and age that we can format into keywords (tags) in the ScienceBase Items, which exposes the terms for faceted search and other uses. It also sets up a conversation about semantic alignment when we look at the shorthand form of subsurface geologic formation and age present in much of the interval information from the CRC records, which I will also digest into tags from a different scheme/vocabulary.\n",
    "\n",
    "Similar to the situation with web scraping, I ran into fewer but still some hiccups in accessing the Macrostrat API. Since there are some point coordinates in the data that are the same, and we are only retrieving data for each unique point, I created document stubs of only the unique coordinates, put them in a ledger to be filled, and run them in a loop until I can fill every order. For convenience, I use geohash2 here to decode the hashes to latitude and longitude for the ledger as I need to pass them as separate variables to the Macrostrat API that I'm using.\n",
    "\n",
    "# Dependencies\n",
    "This code requires the Requests, Pymongo, and Geohash2 packages, all installed from Conda-Forge distributions in my case. Processing data at even this relatively small scale on a point by point basis really does require a database of some kind to deal with reasonably. I use MongoDB in this case, but it could be anything. The Macrostrat API used is described at its logical path [here](https://macrostrat.org/api/mobile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import geohash2\n",
    "\n",
    "mongo_ndc = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records with point coordinates: 69899\n",
      "Total unique geohashes: 58544\n",
      "CPU times: user 2.65 s, sys: 36.1 ms, total: 2.69 s\n",
      "Wall time: 3.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1040cf648>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Get coordinates from cores\n",
    "all_coords = [i[\"coordinates_geohash\"] for i in mongo_ndc.crc.cores_raw.find({\"coordinates_geohash\": {\"$ne\": None}},{\"coordinates_geohash\": 1})]\n",
    "# Extend to add coordinates from cuttings\n",
    "all_coords.extend([i[\"coordinates_geohash\"] for i in mongo_ndc.crc.cuttings_raw.find({\"coordinates_geohash\": {\"$ne\": None}},{\"coordinates_geohash\": 1})])\n",
    "print(\"Total records with point coordinates:\", len(all_coords))\n",
    "# Reduce the list by limiting to unique geohashes\n",
    "unique_coords = list(set(all_coords))\n",
    "# Build the ledger\n",
    "unique_coords_ledger = [\n",
    "    {\n",
    "        \"coordinates_geohash\": c,\n",
    "        \"coordinates\": [\n",
    "            geohash2.decode(c)[1],\n",
    "            geohash2.decode(c)[0]\n",
    "        ]\n",
    "    } for c in unique_coords\n",
    "]\n",
    "print(\"Total unique geohashes:\", len(unique_coords_ledger))\n",
    "# Insert the ledger into MongoDB to be filled\n",
    "mongo_ndc.crc.gmu_context.insert_many(unique_coords_ledger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surficial Geology Context\n",
    "The Macrostrat team did the heavy lifting of integrating multiple resolutions of geologic maps from different national and global sources and setting up an API for retrieving this basic surface geology context with (nearly) any point location. Information retrieved from this API that may be of interest in the National Digital Catalog for narrowing search in a faceted manner include rock type, geologic formation, and stratigraphic units. The following function handles the basic process of requesting this information for a set of coordinates, stamping the result with datetime, and returning the information for our ledger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macrostrat_context(coordinates):\n",
    "    api = f\"https://macrostrat.org/api/mobile/point?lat={coordinates[1]}&lng={coordinates[0]}\"\n",
    "    \n",
    "    r = requests.get(api, headers={\"accept\": \"application/json\"}).json()\n",
    "    \n",
    "    if \"success\" in r.keys() and \"data\" in r[\"success\"].keys():\n",
    "        return {\n",
    "            \"date_retrieved\": datetime.utcnow().isoformat(),\n",
    "            \"data\": r[\"success\"][\"data\"]\n",
    "        }\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is prepped, we can run through and request what we need from the Macrostrat API. Even though we could multithread this and get it done faster, I don't know what kind of denial of service thresholds may be in place for the API. If we end up wanting to run this as a production enhancer for every point coordinate in the NDC, we would also want to set up some different kind of process as this really won't scale to that level. Depending on how the geologic map unit polygon data is set up (e.g., something like an Elasticsearch index), we should be able to simply pass what might be a very large collection of geohashed points, find all polygons containing those points, and then return relevant properties from the index. In the meantime, we can look at the available information as tags within ScienceBase and discuss its utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = mongo_ndc.crc.gmu_context.find_one({\"date_retrieved\": {\"$exists\": False}})\n",
    "while item is not None:\n",
    "    mongo_ndc.crc.gmu_context.update_one({\"_id\": item[\"_id\"]},{\"$set\": macrostrat_context(tuple(item[\"coordinates\"]))})\n",
    "    item = mongo_ndc.crc.gmu_context.find_one({\"date_retrieved\": {\"$exists\": False}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
